from flask import Flask,render_template
import os
import psycopg2
from psycopg2 import OperationalError, ProgrammingError, DatabaseError, InterfaceError, IntegrityError, Error
from dotenv import load_dotenv

from flask import Flask, render_template, request, Response, jsonify
import pandas as pd
import numpy as np
import joblib # joblib for .pkl
import json
import os
import time
import traceback
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut, GeocoderServiceError
from sklearn.metrics import mean_absolute_error

# --- å°å…¥æ‰€æœ‰å¿…è¦çš„APIæŸ¥è©¢å·¥å…·å’Œå…¨å±€è®Šæ•¸ ---
try:
    from geopy.geocoders import Nominatim
    from geopy.exc import GeocoderTimedOut, GeocoderServiceError
except ImportError:
    print("âŒ éŒ¯èª¤ï¼šç¼ºå°‘ geopy å¥—ä»¶ï¼è«‹åŸ·è¡Œ pip install geopy")
    exit()

# --- åˆå§‹åŒ– Flask æ‡‰ç”¨ç¨‹å¼ ---
app = Flask(__name__)

# --- å…¨å±€è®Šæ•¸ ---
MODEL_PERFORMANCE = {
    "mae": "N/A", # é è¨­å€¼ï¼Œä»¥é˜²æ¸¬è©¦æ•¸æ“šè¼‰å…¥å¤±æ•—
    "mae_total_price_example": "N/A"
}
blended_model = None
poi_database = None      # <--- åˆå§‹åŒ–ç‚º None
feature_defaults = None  # <--- åˆå§‹åŒ–ç‚º None
geolocator = None

# --- ã€é‡å¤§ä¿®æ”¹é»ï¼šå‰µå»ºä¸€å€‹ BlendedModel é¡ä¾†ç®¡ç†èåˆé‚è¼¯ã€‘ ---
class BlendedModel:
    def __init__(self, config_filepath):
        print(f"--- æ­£åœ¨è¼‰å…¥èåˆæ¨¡å‹é…ç½®: {config_filepath} ---")
        
        if not os.path.exists(config_filepath):
            raise FileNotFoundError(f"èåˆæ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_filepath}")
            
        with open(config_filepath, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        self.models = []
        base_dir = os.path.dirname(config_filepath)
        
        print(f"é…ç½®æ–‡ä»¶åŸºç¤ç›®éŒ„: {base_dir}")
        print(f"éœ€è¦åŠ è¼‰ {len(config['models'])} å€‹å­æ¨¡å‹")

        for i, model_info in enumerate(config['models']):
            print(f"\n--- è™•ç†ç¬¬ {i+1} å€‹æ¨¡å‹: {model_info['name']} ---")
            
            # å˜—è©¦å¤šç¨®è·¯å¾‘è§£ææ–¹å¼
            model_path_options = [
                model_info['path'],  # åŸå§‹è·¯å¾‘
                os.path.join(base_dir, model_info['path']),  # ç›¸å°æ–¼é…ç½®æ–‡ä»¶
                os.path.join(base_dir, os.path.basename(model_info['path'])),  # åªå–æ–‡ä»¶å
            ]
            
            model_path = None
            for path_option in model_path_options:
                print(f"  å˜—è©¦è·¯å¾‘: {path_option}")
                if os.path.exists(path_option):
                    model_path = path_option
                    print(f"  âœ… æ‰¾åˆ°æ–‡ä»¶!")
                    break
                else:
                    print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            
            if model_path is None:
                print(f"  ğŸ’¡ å»ºè­°è§£æ±ºæ–¹æ¡ˆ:")
                print(f"     1. æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨æ–¼: {model_info['path']}")
                print(f"     2. æˆ–å°‡æ–‡ä»¶è¤‡è£½åˆ°: {os.path.join(base_dir, os.path.basename(model_info['path']))}")
                print(f"     3. æˆ–ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾‘ç‚ºç›¸å°è·¯å¾‘")
                raise FileNotFoundError(f"èåˆé…ç½®ä¸­çš„å­æ¨¡å‹æª”æ¡ˆæœªæ‰¾åˆ°: {model_info['path']}")
            
            # åŠ è¼‰æ¨¡å‹
            try:
                print(f"  æ­£åœ¨è¼‰å…¥æ¨¡å‹æ–‡ä»¶: {model_path}")
                model = joblib.load(model_path)
                print(f"  âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ (é¡å‹: {type(model)})")
                
                self.models.append({
                    'name': model_info['name'],
                    'model': model,
                    'weight': model_info['weight'],
                    'path': model_path
                })
                
            except Exception as load_error:
                print(f"  âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {load_error}")
                raise RuntimeError(f"ç„¡æ³•è¼‰å…¥æ¨¡å‹ {model_info['name']}: {load_error}")
        
        print(f"\nâœ… æ‰€æœ‰ {len(self.models)} å€‹å­æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
        for model_info in self.models:
            print(f"  - {model_info['name']}: æ¬Šé‡ {model_info['weight']}, è·¯å¾‘ {model_info['path']}")

    def predict(self, X):
        """åŸ·è¡ŒåŠ æ¬Šå¹³å‡é æ¸¬"""
        if not self.models:
            print("âŒ éŒ¯èª¤ï¼šèåˆæ¨¡å‹ä¸­æ²’æœ‰å­æ¨¡å‹å¯ä¾›é æ¸¬ã€‚")
            return np.zeros(len(X))
        
        final_predictions = np.zeros(len(X))
        total_weight = sum(m['weight'] for m in self.models)

        if total_weight == 0:
            print("âš ï¸ è­¦å‘Šï¼šæ¨¡å‹æ¬Šé‡ç¸½å’Œç‚º0ï¼Œå°‡æ¡ç”¨ç°¡å–®å¹³å‡æ³•ã€‚")
            for m_info in self.models:
                final_predictions += m_info['model'].predict(X)
            return final_predictions / len(self.models)
        
        for m_info in self.models:
            model_pred = m_info['model'].predict(X)
            final_predictions += model_pred * m_info['weight']
        
        return final_predictions / total_weight

    def get_expected_features(self):
        """ç²å–æ¨¡å‹æœŸæœ›çš„è¼¸å…¥ç‰¹å¾µåˆ—è¡¨"""
        if not self.models:
            raise AttributeError("ç„¡æ³•å¾èåˆæ¨¡å‹ç²å–æœŸæœ›ç‰¹å¾µï¼šæ²’æœ‰åŠ è¼‰ä»»ä½•å­æ¨¡å‹")
            
        first_model = self.models[0]['model']
        
        if hasattr(first_model, 'named_steps') and 'preprocessor' in first_model.named_steps:
            preprocessor = first_model.named_steps['preprocessor']
            if hasattr(preprocessor, 'feature_names_in_'):
                return preprocessor.feature_names_in_
            elif hasattr(preprocessor, 'transformers_') and preprocessor.transformers_:
                # è™•ç† ColumnTransformer
                all_features = []
                for name, trans, columns in preprocessor.transformers_:
                    if isinstance(columns, str):
                        all_features.append(columns)
                    else:
                        all_features.extend(list(columns))
                return np.array(list(dict.fromkeys(all_features)))
        
        # å¦‚æœæ˜¯ç°¡å–®çš„æ¨¡å‹ï¼Œå˜—è©¦å…¶ä»–å±¬æ€§
        if hasattr(first_model, 'feature_names_in_'):
            return first_model.feature_names_in_
        
        raise AttributeError("ç„¡æ³•å¾èåˆæ¨¡å‹ä¸­ç²å–æœŸæœ›çš„ç‰¹å¾µåˆ—è¡¨")


# --- åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚ï¼Œä¸€æ¬¡æ€§è¼‰å…¥æ‰€æœ‰å¿…è¦çš„éœæ…‹è³‡æº ---
# --- åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚ï¼Œä¸€æ¬¡æ€§è¼‰å…¥æ‰€æœ‰å¿…è¦çš„éœæ…‹è³‡æº ---
try:
    MODEL_CONFIG_PATH = os.path.join('models', 'FINAL_blended_config.json')
    POI_DB_PATH = os.path.join('models', 'poi_database.pkl')
    DEFAULTS_PATH = os.path.join('models', 'feature_defaults.json')
    TEST_DATA_PATH = os.path.join('data', 'FINAL_TEST_2024-2025.parquet')

    print("=== é–‹å§‹åŠ è¼‰éœæ…‹è³‡æº ===")
    
    # ã€æ­¥é©Ÿ1ã€‘æª¢æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    required_files = {
        'MODEL_CONFIG': MODEL_CONFIG_PATH,
        'POI_DB': POI_DB_PATH,
        'DEFAULTS': DEFAULTS_PATH,
        'TEST_DATA': TEST_DATA_PATH
    }
    
    for name, path in required_files.items():
        if os.path.exists(path):
            print(f"âœ… {name}: {path}")
        else:
            print(f"âŒ {name}: {path} - æ–‡ä»¶ä¸å­˜åœ¨ï¼")
    
    # ã€æ­¥é©Ÿ2ã€‘åŠ è¼‰ POI æ•¸æ“šåº«
    print("æ­£åœ¨è¼‰å…¥ POI è³‡æ–™åº«...")
    if os.path.exists(POI_DB_PATH):
        poi_database = joblib.load(POI_DB_PATH)
        print(f"âœ… POI è³‡æ–™åº«è¼‰å…¥æˆåŠŸ (é¡å‹: {type(poi_database)})")
    else:
        raise FileNotFoundError(f"POI è³‡æ–™åº«æª”æ¡ˆæœªæ‰¾åˆ°: {POI_DB_PATH}")

    # ã€æ­¥é©Ÿ3ã€‘åŠ è¼‰ç‰¹å¾µé»˜èªå€¼
    print("æ­£åœ¨è¼‰å…¥ç‰¹å¾µé è¨­å€¼...")
    if os.path.exists(DEFAULTS_PATH):
        with open(DEFAULTS_PATH, 'r', encoding='utf-8') as f:
            feature_defaults = json.load(f)
        print(f"âœ… ç‰¹å¾µé è¨­å€¼è¼‰å…¥æˆåŠŸ (ç‰¹å¾µæ•¸: {len(feature_defaults)})")
    else:
        raise FileNotFoundError(f"ç‰¹å¾µé è¨­å€¼æª”æ¡ˆæœªæ‰¾åˆ°: {DEFAULTS_PATH}")
    
    # ã€æ­¥é©Ÿ4ã€‘åŠ è¼‰èåˆæ¨¡å‹
    print("æ­£åœ¨è¼‰å…¥èåˆæ¨¡å‹...")
    blended_model = BlendedModel(MODEL_CONFIG_PATH)
    print("âœ… èåˆæ¨¡å‹è¼‰å…¥æˆåŠŸ")

    # ã€æ­¥é©Ÿ5ã€‘è©•ä¼°æ¨¡å‹æ€§èƒ½
    print("æ­£åœ¨è©•ä¼°æ¨¡å‹æ€§èƒ½...")
    if os.path.exists(TEST_DATA_PATH):
        try:
            df_test = pd.read_parquet(TEST_DATA_PATH)
            target_col = 'å»ºç‰©å–®åƒ¹_å…ƒåª'
            
            expected_features = blended_model.get_expected_features()
            X_test = df_test[expected_features].copy()
            y_test_true = df_test[target_col]
            
            y_test_pred = blended_model.predict(X_test)
            mae_value = mean_absolute_error(y_test_true, y_test_pred)
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ¨™
            MODEL_PERFORMANCE['mae'] = f"{mae_value:,.2f}"
            MODEL_PERFORMANCE['mae_total_price_example'] = f"{(mae_value * 30):,.0f}"
            
            print(f"âœ… æ¨¡å‹æ€§èƒ½è©•ä¼°å®Œæˆ: MAE = {mae_value:.2f}")
            print(f"   -> æ›´æ–°å¾Œçš„ MODEL_PERFORMANCE: {MODEL_PERFORMANCE}")
            
        except Exception as perf_error:
            print(f"âš ï¸ æ¨¡å‹æ€§èƒ½è©•ä¼°å¤±æ•—: {perf_error}")
            print("   -> MODEL_PERFORMANCE å°‡ä¿æŒé»˜èªå€¼")
            # ä¸æ‹‹å‡ºç•°å¸¸ï¼Œè®“æ‡‰ç”¨ç¹¼çºŒé‹è¡Œ
    else:
        print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ¸¬è©¦é›†æª”æ¡ˆ {TEST_DATA_PATH}ï¼Œç„¡æ³•è¨ˆç®—æ¨¡å‹æ€§èƒ½æŒ‡æ¨™ã€‚")
        print("   -> MODEL_PERFORMANCE å°‡ä¿æŒé»˜èªå€¼")

    # ã€æ­¥é©Ÿ6ã€‘åˆå§‹åŒ–åœ°ç†ç·¨ç¢¼å™¨
    print("æ­£åœ¨åˆå§‹åŒ–åœ°ç†ç·¨ç¢¼å™¨...")
    geolocator = Nominatim(user_agent=f"RealEstateWebApp/{int(time.time())}")
    print("âœ… åœ°ç†ç·¨ç¢¼å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    print("=== æ‰€æœ‰è³‡æºåŠ è¼‰å®Œæˆ ===")
    print(f"æœ€çµ‚ MODEL_PERFORMANCE: {MODEL_PERFORMANCE}")

except Exception as e:
    print(f"âŒ å•Ÿå‹•éŒ¯èª¤ï¼šè¼‰å…¥æ¨¡å‹æˆ–æ•¸æ“šæ™‚ç™¼ç”Ÿå•é¡Œï¼")
    print(f"éŒ¯èª¤é¡å‹: {type(e).__name__}")
    print(f"éŒ¯èª¤è¨Šæ¯: {str(e)}")
    print("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
    print(traceback.format_exc())
    
    # è¨­ç½®éŒ¯èª¤ç‹€æ…‹
    blended_model = None
    poi_database = None
    feature_defaults = None
    geolocator = None
    
    # æ›´æ–° MODEL_PERFORMANCE ä»¥åæ˜ éŒ¯èª¤ç‹€æ…‹
    MODEL_PERFORMANCE['mae'] = "åŠ è¼‰å¤±æ•—"
    MODEL_PERFORMANCE['mae_total_price_example'] = "ç„¡æ³•è¨ˆç®—"
    
    print(f"éŒ¯èª¤ç‹€æ…‹ä¸‹çš„ MODEL_PERFORMANCE: {MODEL_PERFORMANCE}")
    print("æ‡‰ç”¨ç¨‹å¼å°‡ä»¥å—é™æ¨¡å¼é‹è¡Œï¼ˆç„¡æ³•æä¾›é æ¸¬æœå‹™ï¼‰")


# --- å¾Œç«¯é­”æ³•ï¼šæ ¸å¿ƒè¼”åŠ©å‡½å¼ ---

def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371000
    phi1, phi2 = np.radians(lat1), np.radians(lat2)
    delta_phi = np.radians(lat2 - lat1)
    delta_lambda = np.radians(lon2 - lon1)
    a = np.sin(delta_phi / 2.0)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0)**2
    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

def calculate_geo_features(lat, lon, current_poi_db): # åƒæ•¸åæ”¹ç‚º current_poi_db ä»¥é¿å…èˆ‡å…¨å±€è®Šæ•¸æ··æ·†
    """æ ¹æ“šç¶“ç·¯åº¦å’ŒPOIè³‡æ–™åº«ï¼Œå³æ™‚è¨ˆç®—æ‰€æœ‰åœ°ç†ç©ºé–“ç‰¹å¾µ"""
    geo_features = {}
    
    if not current_poi_db: # æª¢æŸ¥ poi_database æ˜¯å¦å·²æˆåŠŸè¼‰å…¥
        print("âŒ éŒ¯èª¤: POI è³‡æ–™åº«æœªè¼‰å…¥ï¼Œç„¡æ³•è¨ˆç®—åœ°ç†ç‰¹å¾µã€‚")
        return geo_features # è¿”å›ç©ºçš„åœ°ç†ç‰¹å¾µ

    if 'final_locs' in current_poi_db:
        for name, poi_coord in current_poi_db['final_locs'].items():
            feature_name = f'è·é›¢_{name}'
            geo_features[feature_name] = haversine_distance(lat, lon, poi_coord[0], poi_coord[1])
    
    if 'loc_density' in current_poi_db:
        for name, coords_list in current_poi_db['loc_density'].items():
            feature_name = f'{name.replace(" ","_")}æ•¸é‡_300m'
            if not coords_list:
                geo_features[feature_name] = 0
                continue
            # ç¢ºä¿ coords_list ä¸­çš„åº§æ¨™æ˜¯ (lat, lon) å°
            if coords_list and isinstance(coords_list[0], (list, tuple)) and len(coords_list[0]) == 2:
                distances = [haversine_distance(lat, lon, p_lat, p_lon) for p_lat, p_lon in coords_list]
                geo_features[feature_name] = int(np.sum(np.array(distances) <= 300))
            else:
                print(f"âš ï¸ è­¦å‘Š: POIå¯†åº¦æ•¸æ“š '{name}' æ ¼å¼ä¸æ­£ç¢ºï¼Œè·³éè¨ˆç®—ã€‚æ‡‰ç‚ºåº§æ¨™åˆ—è¡¨ã€‚")
                geo_features[feature_name] = 0
            
    all_cats = ['æ·é‹', 'æ©‹æ¨‘', 'å­¸å€', 'æ–‡æ•™', 'é‹å‹•', 'å¸‚å ´', 'ä¸»è¦å¹¹é“', 'æœªä¾†æ·é‹', 'è­¦å¯Ÿå±€', 'å…¬åœ’', 'è³¼ç‰©', 'è¡Œæ”¿', 'å«Œæƒ¡_å®®å»Ÿ']
    for cat in all_cats:
        cat_distances = [v for k, v in geo_features.items() if k.startswith(f'è·é›¢_{cat}_')]
        if cat_distances:
            geo_features[f'è·é›¢_æœ€è¿‘{cat}'] = min(cat_distances)
        else:
            # ç¢ºä¿ feature_defaults å·²æˆåŠŸè¼‰å…¥
            default_val = 99999 # ç¡¬ç·¨ç¢¼çš„æœ€çµ‚å‚™ç”¨å€¼
            if feature_defaults and f'è·é›¢_æœ€è¿‘{cat}' in feature_defaults:
                 default_val = feature_defaults[f'è·é›¢_æœ€è¿‘{cat}']
            else:
                print(f"âš ï¸ è­¦å‘Š: ç‰¹å¾µ 'è·é›¢_æœ€è¿‘{cat}' åœ¨ feature_defaults ä¸­æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç¡¬ç·¨ç¢¼é è¨­å€¼ {default_val}ã€‚")
            geo_features[f'è·é›¢_æœ€è¿‘{cat}'] = default_val


    return geo_features

def build_feature_vector(user_input, geo_features, current_defaults, expected_features_list):
    """æ§‹å»ºæœ€çµ‚ç”¨æ–¼é æ¸¬çš„ã€å®Œæ•´çš„ç‰¹å¾µå‘é‡ (v2 - å¢å¼·ç‰ˆ)"""
    print("--- é–‹å§‹æ§‹å»ºç‰¹å¾µå‘é‡ ---")
    if not current_defaults: # æª¢æŸ¥ feature_defaults æ˜¯å¦å·²æˆåŠŸè¼‰å…¥
        print("âŒ éŒ¯èª¤: ç‰¹å¾µé è¨­å€¼æœªè¼‰å…¥ï¼Œç„¡æ³•æ§‹å»ºç‰¹å¾µå‘é‡ã€‚")
        # å³ä½¿é è¨­å€¼æœªè¼‰å…¥ï¼Œæˆ‘å€‘ä»ç„¶å˜—è©¦å‰µå»ºä¸€å€‹ç©ºçš„ DataFrame
        # ä½†é€™å¾ˆå¯èƒ½æœƒå°è‡´å¾ŒçºŒçš„éŒ¯èª¤
        return pd.DataFrame(columns=expected_features_list)

    vector = current_defaults.copy()
    
    for key, value in user_input.items():
        if value not in [None, ""] and key in vector:
            try:
                vector[key] = float(value)
            except (ValueError, TypeError):
                vector[key] = value # ä¿æŒåŸæ¨£ (ä¾‹å¦‚æ–‡å­—å‹ç‰¹å¾µ)
    
    vector.update(geo_features)
    
    # å‰µå»ºä¸€å€‹Seriesï¼Œå…¶ç´¢å¼•èˆ‡ expected_features_list åŒ¹é…ï¼Œä¸¦ç”¨ vector ä¸­çš„å€¼å¡«å……
    # é€™æ¨£å¯ä»¥ç¢ºä¿é †åºå’Œæ¬„ä½åç¨±éƒ½æ­£ç¢º
    # å°æ–¼ vector ä¸­æ²’æœ‰ï¼Œä½† expected_features_list ä¸­æœ‰çš„ç‰¹å¾µï¼Œæœƒæ˜¯ NaN
    # å°æ–¼ vector ä¸­æœ‰ï¼Œä½† expected_features_list ä¸­æ²’æœ‰çš„ç‰¹å¾µï¼Œæœƒè¢«å¿½ç•¥
    # ï¼ˆå¾Œè€…ä¸æ‡‰è©²ç™¼ç”Ÿï¼Œå¦‚æœ expected_features_list æ˜¯å…¨é¢çš„ï¼‰
    
    # å…ˆå¾ vector å‰µå»ºä¸€å€‹ Series
    temp_series = pd.Series(vector)
    # æ ¹æ“š expected_features_list é‡æ–°ç´¢å¼•ï¼Œç¼ºå¤±çš„ç”¨ NaN å¡«å……
    # é€™è£¡çš„ reindex å¾ˆé‡è¦ï¼Œå®ƒç¢ºä¿äº†æ¬„ä½çš„é †åºå’Œæ¨¡å‹æœŸæœ›çš„ä¸€è‡´
    final_series = temp_series.reindex(expected_features_list)


    # å°‡ Series è½‰æ›ç‚º DataFrame (å–®è¡Œ)
    final_df = pd.DataFrame([final_series])


    # --- è¨ˆç®—äº¤å‰ç‰¹å¾µ ---
    # ç¢ºä¿é€™äº›åŸºç¤ç‰¹å¾µå­˜åœ¨æ–¼ final_df ä¸­ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­å€¼æˆ–0
    # æ³¨æ„ï¼šç¾åœ¨ final_df çš„æ¬„ä½é †åºå·²ç¶“ç”± expected_features_list æ±ºå®š
    # æˆ‘å€‘éœ€è¦ç¢ºä¿ç”¨æ–¼è¨ˆç®—äº¤å‰ç‰¹å¾µçš„åŸºç¤æ¬„ä½åœ¨ final_df ä¸­å­˜åœ¨
    
    # ç¤ºä¾‹äº¤å‰ç‰¹å¾µï¼šæœ€çµ‚_å±‹é½¡_X_æ¨“å±¤æ¯”
    # ç¢ºä¿ 'æœ€çµ‚_å±‹é½¡' å’Œ 'æ¨“å±¤æ¯”' åœ¨ current_defaults ä¸­æœ‰åˆç†çš„é è¨­å€¼
    å±‹é½¡ = final_df.loc[0, 'æœ€çµ‚_å±‹é½¡'] if 'æœ€çµ‚_å±‹é½¡' in final_df.columns else current_defaults.get('æœ€çµ‚_å±‹é½¡', 0)
    æ¨“å±¤æ¯” = final_df.loc[0, 'æ¨“å±¤æ¯”'] if 'æ¨“å±¤æ¯”' in final_df.columns else current_defaults.get('æ¨“å±¤æ¯”', 0)
    
    # å¦‚æœäº¤å‰ç‰¹å¾µæ¬„ä½æœ¬èº«åœ¨æ¨¡å‹æœŸæœ›çš„ç‰¹å¾µåˆ—è¡¨ä¸­
    if 'æœ€çµ‚_å±‹é½¡_X_æ¨“å±¤æ¯”' in final_df.columns:
        final_df.loc[0, 'æœ€çµ‚_å±‹é½¡_X_æ¨“å±¤æ¯”'] = å±‹é½¡ * æ¨“å±¤æ¯”
    # ... (æ·»åŠ æ‰€æœ‰å…¶ä»–äº¤å‰ç‰¹å¾µçš„è¨ˆç®—é‚è¼¯) ...
    # ç¤ºä¾‹:
    # if 'ä¸»å»ºç‰©åªæ•¸_X_æ¨“å±¤æ¯”' in final_df.columns:
    #     ä¸»å»ºç‰©åªæ•¸ = final_df.loc[0, 'ä¸»å»ºç‰©åªæ•¸'] if 'ä¸»å»ºç‰©åªæ•¸' in final_df.columns else current_defaults.get('ä¸»å»ºç‰©åªæ•¸',0)
    #     final_df.loc[0, 'ä¸»å»ºç‰©åªæ•¸_X_æ¨“å±¤æ¯”'] = ä¸»å»ºç‰©åªæ•¸ * æ¨“å±¤æ¯”

    # ã€é‡è¦ã€‘å†æ¬¡ä½¿ç”¨ current_defaults å° NaN å€¼é€²è¡Œå¡«å……ï¼Œä»¥è™•ç† reindex å¯èƒ½ç”¢ç”Ÿçš„ NaN
    # æˆ–è€…è™•ç†äº¤å‰ç‰¹å¾µè¨ˆç®—ä¸­ï¼Œå› åŸºç¤ç‰¹å¾µç¼ºå¤±è€Œå°è‡´çš„ NaN
    # åªå¡«å…… final_df ä¸­å¯¦éš›å­˜åœ¨çš„æ¬„ä½
    for col in final_df.columns:
        if final_df[col].isnull().any():
            if col in current_defaults:
                final_df[col].fillna(current_defaults[col], inplace=True)
            else:
                # å¦‚æœæŸå€‹æœŸæœ›çš„æ¬„ä½åœ¨ defaults ä¸­ä¹Ÿæ²’æœ‰ï¼Œé€™æ˜¯ä¸€å€‹æ½›åœ¨å•é¡Œ
                # é€™è£¡å¯ä»¥é¸æ“‡å¡«å……ä¸€å€‹é€šç”¨å€¼ (å¦‚0) æˆ–ç™¼å‡ºè­¦å‘Š
                print(f"âš ï¸ è­¦å‘Š: æ¬„ä½ '{col}' åœ¨ defaults ä¸­æ²’æœ‰é è¨­å€¼ï¼ŒNaN å€¼å¯èƒ½æœªè¢«è™•ç†ã€‚")
                final_df[col].fillna(0, inplace=True) # å‚™ç”¨å¡«å……

    print(f"--- ç‰¹å¾µå‘é‡æ§‹å»ºå®Œæˆ (æ¬„ä½æ•¸: {len(final_df.columns)}) ---")
    return final_df



load_dotenv()  # è®€å– .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸
conn_string = os.getenv('RENDER_DATABASE')

app = Flask(__name__)

@app.route("/")
def index():
    # è©³ç´°çš„èª¿è©¦ä¿¡æ¯
    print("="*50)
    print("DEBUG: é€²å…¥ index è·¯ç”±")
    print(f"DEBUG: MODEL_PERFORMANCE = {MODEL_PERFORMANCE}")
    print(f"DEBUG: blended_model æ˜¯å¦å·²åŠ è¼‰: {blended_model is not None}")
    print(f"DEBUG: poi_database æ˜¯å¦å·²åŠ è¼‰: {poi_database is not None}")
    print(f"DEBUG: feature_defaults æ˜¯å¦å·²åŠ è¼‰: {feature_defaults is not None}")
    print("="*50)
    
    # æ§‹å»ºå‚³éçµ¦æ¨¡æ¿çš„æ•¸æ“š
    template_data = {
        'performance': MODEL_PERFORMANCE,
        'model_loaded': blended_model is not None,
        'debug_info': {
            'mae_value': MODEL_PERFORMANCE['mae'],
            'total_price_example': MODEL_PERFORMANCE['mae_total_price_example']
        }
    }
    
    return render_template('index.html', **template_data)

@app.route("/index")
def index2():
    return render_template("index.html.jinja2")


@app.route("/classes", defaults={'course_type': 'ä¸€èˆ¬èª²ç¨‹'})
@app.route("/classes/<course_type>")
def classes(course_type):
    print(f"ç›®å‰é¡¯ç¤ºçš„èª²ç¨‹é¡åˆ¥æ˜¯: {course_type}")

    # all_course_categories ç”¨æ–¼å°è¦½åˆ—æˆ–ç¯©é¸å™¨ (ä¾‹å¦‚é¡¯ç¤ºæ‰€æœ‰èª²ç¨‹é¡åˆ¥çš„åˆ—è¡¨)
    all_course_categories = []
    # courses_for_cards ç”¨æ–¼å¡ç‰‡é¡¯ç¤ºï¼Œæœƒæ˜¯å­—å…¸åˆ—è¡¨
    courses_for_cards = []

    try:
        with psycopg2.connect(conn_string) as conn:
            with conn.cursor() as cur:
                # æ­¥é©Ÿ 1: æŸ¥è©¢æ‰€æœ‰ç¨ç‰¹çš„èª²ç¨‹é¡åˆ¥ (å¦‚æœä½ çš„é é¢æœ‰é¡åˆ¥ç¯©é¸åŠŸèƒ½ï¼Œé€™æœƒå¾ˆæœ‰ç”¨)
                sql_get_categories = """
                SELECT DISTINCT "èª²ç¨‹é¡åˆ¥" FROM "é€²ä¿®èª²ç¨‹";
                """
                cur.execute(sql_get_categories)
                raw_categories = cur.fetchall()
                all_course_categories = [cat[0] for cat in raw_categories]
                all_course_categories.reverse() # å¦‚æœä½ éœ€è¦åå‘æ’åº

                # æ­¥é©Ÿ 2: æ ¹æ“šé¸å®šçš„ course_type æŸ¥è©¢è©³ç´°çš„èª²ç¨‹è³‡æ–™
                sql_get_courses = """
                SELECT
                    "èª²ç¨‹åç¨±",
                    "è€å¸«",
                    "é€²ä¿®äººæ•¸",
                    "å ±åé–‹å§‹æ—¥æœŸ",
                    "å ±åçµæŸæ—¥æœŸ",
                    "èª²ç¨‹å…§å®¹",
                    "é€²ä¿®è²»ç”¨"
                FROM
                    "é€²ä¿®èª²ç¨‹"
                WHERE
                    "èª²ç¨‹é¡åˆ¥" = %s -- ä½¿ç”¨åƒæ•¸åŒ–æŸ¥è©¢ï¼Œé˜²æ­¢ SQL æ³¨å…¥
                ORDER BY "å ±åé–‹å§‹æ—¥æœŸ" DESC -- å¯ä»¥åŠ ä¸Šæ’åºï¼Œä¾‹å¦‚æŒ‰æ—¥æœŸé™åº
                LIMIT 6; -- é™åˆ¶é¡¯ç¤ºçš„å¡ç‰‡æ•¸é‡ï¼Œé€™è£¡é™åˆ¶ç‚º 6 å¼µ
                """
                # æ³¨æ„ï¼šå‚³éçµ¦ execute çš„åƒæ•¸å¿…é ˆæ˜¯å…ƒçµ„ (å³ä½¿åªæœ‰ä¸€å€‹åƒæ•¸)
                cur.execute(sql_get_courses, (course_type,))
                raw_course_rows = cur.fetchall()

                # å–å¾—æŸ¥è©¢çµæœçš„æ¬„ä½åç¨±ï¼Œç”¨æ–¼å°‡å…ƒçµ„è½‰æ›ç‚ºå­—å…¸
                column_names = [desc[0] for desc in cur.description]

                # å°‡æ¯ä¸€è¡Œèª²ç¨‹è³‡æ–™ (å…ƒçµ„) è½‰æ›ç‚ºå­—å…¸
                for row in raw_course_rows:
                    course_dict = {}
                    for i, col_name in enumerate(column_names):
                        # å°‡è³‡æ–™åº«æ¬„ä½åç¨±æ˜ å°„åˆ° Jinja æ¨¡æ¿ä¸­ä½¿ç”¨çš„éµå
                        if col_name == "èª²ç¨‹åç¨±":
                            course_dict['title'] = row[i]
                        elif col_name == "èª²ç¨‹å…§å®¹":
                            course_dict['body'] = row[i]
                        elif col_name == "è€å¸«":
                            course_dict['teacher'] = row[i]
                        elif col_name == "é€²ä¿®è²»ç”¨":
                            course_dict['price'] = row[i]
                        # å¯ä»¥æ ¹æ“šä½ çš„éœ€æ±‚ï¼Œç¹¼çºŒæ˜ å°„å…¶ä»–æ¬„ä½
                        # æ¯”å¦‚æ—¥æœŸå¯ä»¥æ ¼å¼åŒ–ï¼Œäººæ•¸å¯èƒ½éœ€è¦è½‰æ›ç­‰ç­‰
                        # ä¾‹å¦‚:
                        # elif col_name == "å ±åé–‹å§‹æ—¥æœŸ":
                        #     course_dict['start_date'] = row[i].strftime('%Y-%m-%d') if row[i] else None
                        else:
                            course_dict[col_name] = row[i] # å…¶ä»–æ¬„ä½å¦‚æœä¸éœ€è¦ç‰¹åˆ¥æ˜ å°„ï¼Œå°±ä¿æŒåŸå

                    courses_for_cards.append(course_dict)

                print("å¾è³‡æ–™åº«ç²å–çš„èª²ç¨‹é¡åˆ¥ (all_course_categories):", all_course_categories)
                print("å¾è³‡æ–™åº«ç²å–çš„èª²ç¨‹è³‡æ–™ (courses_for_cards):", courses_for_cards)

    except OperationalError as e:
        print(f"ğŸš¨ é€£ç·šå¤±æ•—ï¼šä¼ºæœå™¨æœªå•Ÿå‹•ã€ç¶²è·¯å•é¡Œæˆ–åƒæ•¸éŒ¯èª¤ | è©³ç´°è¨Šæ¯ï¼š{e}")
        return render_template("error.html.jinja2", error_message=f"ğŸš¨ é€£ç·šå¤±æ•—ï¼šä¼ºæœå™¨æœªå•Ÿå‹•ã€ç¶²è·¯å•é¡Œæˆ–åƒæ•¸éŒ¯èª¤ | è©³ç´°è¨Šæ¯ï¼š{e}"), 500
    except InterfaceError as e:
        print(f"ğŸš¨ é€£ç·šä¸­æ–·ï¼šé€£ç·šè¢«æ„å¤–é—œé–‰ | è©³ç´°è¨Šæ¯ï¼š{e}")
        return render_template("error.html.jinja2", error_message=f"ğŸš¨ é€£ç·šä¸­æ–·ï¼šé€£ç·šè¢«æ„å¤–é—œé–‰ | è©³ç´°è¨Šæ¯ï¼š{e}"), 500
    except Error as e:
        print(f"ğŸš¨ è³‡æ–™åº«éŒ¯èª¤ï¼š{e}")
        return render_template("error.html.jinja2", error_message=f"ğŸš¨ è³‡æ–™åº«éŒ¯èª¤ï¼š{e}"), 500
    except Exception as e:
        print(f"ğŸš¨ å…¶ä»–éŒ¯èª¤ï¼š{e}")
        return render_template("error.html.jinja2", error_message=f"ğŸš¨ å…¶ä»–éŒ¯èª¤ï¼š{e}"), 500

    # å°‡è™•ç†å¥½çš„è³‡æ–™å‚³éçµ¦ Jinja æ¨¡æ¿
    return render_template(
        "classes.html.jinja2",
        kinds=all_course_categories,    # æ‰€æœ‰èª²ç¨‹é¡åˆ¥ (ç”¨æ–¼å°è¦½åˆ—/ç¯©é¸å™¨)
        course_data=courses_for_cards,  # å¯¦éš›è¦é¡¯ç¤ºçš„èª²ç¨‹å¡ç‰‡è³‡æ–™
        course_type=course_type         # ç•¶å‰é¸ä¸­çš„èª²ç¨‹é¡åˆ¥ï¼Œç”¨æ–¼åœ¨å°è¦½åˆ—ä¸­é«˜äº®é¡¯ç¤º
    )


@app.route("/news")
def news():
    try:
        with psycopg2.connect(conn_string) as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT * FROM æœ€æ–°è¨Šæ¯ ORDER BY ç·¨è™Ÿ ASC;")
                rows = cur.fetchall()
                colnames = [desc[0] for desc in cur.description]  # å–å¾—æ¬„ä½åç¨±

    except OperationalError as e:
        print(f"ğŸš¨ é€£ç·šå¤±æ•—ï¼šä¼ºæœå™¨æœªå•Ÿå‹•ã€ç¶²è·¯å•é¡Œæˆ–åƒæ•¸éŒ¯èª¤ | è©³ç´°è¨Šæ¯ï¼š{e}")
        return render_template("error.html.jinja2",error_message=f"ğŸš¨ é€£ç·šå¤±æ•—ï¼šä¼ºæœå™¨æœªå•Ÿå‹•ã€ç¶²è·¯å•é¡Œæˆ–åƒæ•¸éŒ¯èª¤ | è©³ç´°è¨Šæ¯ï¼š{e}"), 500
    except InterfaceError as e:
        print(f"ğŸš¨ é€£ç·šä¸­æ–·ï¼šé€£ç·šè¢«æ„å¤–é—œé–‰ | è©³ç´°è¨Šæ¯ï¼š{e}")
        return render_template("error.html.jinja2",error_message=f"ğŸš¨ é€£ç·šä¸­æ–·ï¼šé€£ç·šè¢«æ„å¤–é—œé–‰ | è©³ç´°è¨Šæ¯ï¼š{e}"), 500
    except Error as e:
        print(f"ğŸš¨ è³‡æ–™åº«éŒ¯èª¤ï¼š{e}")
        return render_template("error.html.jinja2",error_message=f"ğŸš¨ è³‡æ–™åº«éŒ¯èª¤ï¼š{e}"), 500
    except Exception as e:
        print(f"ğŸš¨ å…¶ä»–éŒ¯èª¤ï¼š{e}")
        return render_template("error.html.jinja2",error_message=f"ğŸš¨ å…¶ä»–éŒ¯èª¤ï¼š{e}"), 500

    return render_template("news.html.jinja2", rows=rows, colnames=colnames)


@app.route("/traffic")
def traffic():
    return render_template("traffic.html.jinja2")

@app.route("/contact")
def contact():
    return render_template("contact.html.jinja2")

@app.route("/test")
def test():
    return render_template("testindex.html.jinja2")


@app.route('/predict', methods=['POST'])
def predict_route(): # è·¯ç”±å‡½å¼åç¨±ä¿æŒ predict_route
    # åŸ·è¡Œåš´æ ¼çš„åˆå§‹æª¢æŸ¥
    if not blended_model:
        app.logger.error("Predict attempt failed: Blended model not loaded.")
        return jsonify({'error': 'æ¨¡å‹æœªèƒ½æˆåŠŸè¼‰å…¥æˆ–åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•æä¾›é æ¸¬æœå‹™ã€‚'}), 500
    if not poi_database: # æª¢æŸ¥ poi_database æ˜¯å¦ç‚º None æˆ–ç©º
        app.logger.error("Predict attempt failed: POI database not loaded or empty.")
        return jsonify({'error': 'POIè³‡æ–™åº«æœªèƒ½æˆåŠŸè¼‰å…¥æˆ–ç‚ºç©ºï¼Œç„¡æ³•æä¾›é æ¸¬æœå‹™ã€‚'}), 500
    if not feature_defaults:
        app.logger.error("Predict attempt failed: Feature defaults not loaded.")
        return jsonify({'error': 'ç‰¹å¾µé è¨­å€¼æœªèƒ½æˆåŠŸè¼‰å…¥ï¼Œç„¡æ³•æä¾›é æ¸¬æœå‹™ã€‚'}), 500
    if not geolocator:
        app.logger.error("Predict attempt failed: Geolocator not initialized.")
        return jsonify({'error': 'åœ°ç†ç·¨ç¢¼å™¨æœªèƒ½æˆåŠŸåˆå§‹åŒ–ï¼Œç„¡æ³•æä¾›é æ¸¬æœå‹™ã€‚'}), 500

    # ã€é—œéµä¿®æ”¹ã€‘: åœ¨ predict_route çš„ä½œç”¨åŸŸå…§æ•ç² poi_database çš„ç•¶å‰åƒè€ƒ
    # é€™æ¨£ generate_progress é€éé–‰åŒ…è¨ªå•çš„æ˜¯é€™å€‹ç©©å®šçš„åƒè€ƒ
    stable_poi_database_ref = poi_database
    stable_feature_defaults_ref = feature_defaults # åŒæ¨£ç‚º feature_defaults åšæ­¤è™•ç†

    data = request.get_json()
    address = data.get('address')
    user_features = data.get('features', {})

    if not address:
        return jsonify({'error': 'åœ°å€ç‚ºå¿…å¡«é …ç›®ï¼'}), 400

    def generate_progress():
        """ä¸€å€‹ç”Ÿæˆå™¨å‡½å¼ï¼Œåˆ†æ­¥åŸ·è¡Œä»»å‹™ä¸¦ç”¨yieldè¿”å›é€²åº¦"""
        try:
                    # ã€ã€ã€æ–°å¢é™¤éŒ¯æ‰“å°ã€‘ã€‘ã€‘
            app.logger.debug(f"Inside generate_progress: type of stable_poi_database_ref: {type(stable_poi_database_ref)}")
            app.logger.debug(f"Inside generate_progress: stable_poi_database_ref is None: {stable_poi_database_ref is None}")
            if stable_poi_database_ref:
                app.logger.debug(f"Inside generate_progress: keys in stable_poi_database_ref (first 5): {list(stable_poi_database_ref.keys())[:5] if isinstance(stable_poi_database_ref, dict) else 'Not a dict'}")
            # éšæ®µä¸€ï¼šåœ°ç†ç·¨ç¢¼
            yield f"data: {json.dumps({'status': 'æ­£åœ¨å®šä½åœ°å€...', 'progress': 20})}\n\n"
            location = geolocator.geocode(address, timeout=10, country_codes='TW')
            if not location:
                raise ValueError(f"ç„¡æ³•å®šä½åœ°å€ã€Œ{address}ã€ã€‚è«‹å˜—è©¦è¼¸å…¥æ›´è©³ç´°çš„åœ°å€ï¼Œä¾‹å¦‚åŒ…å«ã€Œæ–°åŒ—å¸‚æ°¸å’Œå€ã€ã€‚")
            lat, lon = location.latitude, location.longitude
            
            # éšæ®µäºŒï¼šç”Ÿæˆåœ°ç†ç‰¹å¾µ
            yield f"data: {json.dumps({'status': 'æ­£åœ¨è¨ˆç®—å‘¨é‚Šç’°å¢ƒç‰¹å¾µ...', 'progress': 40})}\n\n"
            # ã€é—œéµä¿®æ”¹ã€‘: ä½¿ç”¨å¾å¤–éƒ¨ä½œç”¨åŸŸæ•ç²çš„ç©©å®šåƒè€ƒ
            geo_features_calculated = calculate_geo_features(lat, lon, stable_poi_database_ref)

            # éšæ®µä¸‰ï¼šæ§‹å»ºå®Œæ•´ç‰¹å¾µå‘é‡
            yield f"data: {json.dumps({'status': 'æ­£åœ¨æº–å‚™æ•¸æ“šä¸¦è«‹æ±‚æ¨¡å‹...', 'progress': 60})}\n\n"
            
            expected_features_list = blended_model.get_expected_features()
            if not isinstance(expected_features_list, (list, np.ndarray)):
                raise TypeError("get_expected_features() è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„åˆ—è¡¨æˆ–é™£åˆ—ã€‚")
            
            app.logger.debug(f"Model expected features ({len(expected_features_list)}): {sorted(list(expected_features_list))}")
            
            # ã€é—œéµä¿®æ”¹ã€‘: ä½¿ç”¨å¾å¤–éƒ¨ä½œç”¨åŸŸæ•ç²çš„ç©©å®šåƒè€ƒ
            input_df = build_feature_vector(user_features, geo_features_calculated, stable_feature_defaults_ref, expected_features_list)

            app.logger.debug(f"Generated features for model ({len(input_df.columns)}): {sorted(list(input_df.columns))}")
            
            set_expected = set(expected_features_list)
            set_generated = set(input_df.columns)

            if set_expected != set_generated:
                missing_in_generated = sorted(list(set_expected - set_generated))
                extra_in_generated = sorted(list(set_generated - set_expected))
                error_msg_parts = ["å¾Œç«¯ç‰¹å¾µç”Ÿæˆèˆ‡æ¨¡å‹æœŸæœ›ä¸ç¬¦ï¼"]
                if missing_in_generated: error_msg_parts.append(f"æ¨¡å‹æœŸæœ›ä½†æœªç”Ÿæˆçš„ç‰¹å¾µ ({len(missing_in_generated)}å€‹): {str(missing_in_generated[:5])}...")
                if extra_in_generated: error_msg_parts.append(f"ç”Ÿæˆäº†ä½†æ¨¡å‹ä¸æœŸæœ›çš„ç‰¹å¾µ ({len(extra_in_generated)}å€‹): {str(extra_in_generated[:5])}...")
                if input_df.isnull().values.any():
                    nan_cols = input_df.columns[input_df.isnull().any()].tolist()
                    error_msg_parts.append(f"è¼¸å…¥æ•¸æ“šä¸­åŒ…å«NaNå€¼ï¼Œåœ¨æ¬„ä½: {nan_cols}")
                app.logger.error("\n".join(error_msg_parts))
                raise ValueError("\n".join(error_msg_parts))
            elif input_df.isnull().values.any():
                nan_cols = input_df.columns[input_df.isnull().any()].tolist()
                app.logger.error(f"Input data contains NaN values in columns: {nan_cols}")
                raise ValueError(f"è¼¸å…¥æ•¸æ“šä¸­åŒ…å«NaNå€¼ï¼Œå³ä½¿ç‰¹å¾µé›†åŒ¹é…ã€‚åœ¨æ¬„ä½: {nan_cols}")
            else:
                app.logger.debug("Feature sets match and no NaN values.")

            yield f"data: {json.dumps({'status': 'æ¨¡å‹é æ¸¬ä¸­...', 'progress': 80})}\n\n"
            prediction_per_ping = blended_model.predict(input_df[list(expected_features_list)])[0] # ç¢ºä¿é †åº
            
            total_price_prediction = "N/A"
            user_ping_str = user_features.get('å»ºç‰©ç¸½åªæ•¸')
            if user_ping_str and str(user_ping_str).strip():
                try:
                    user_ping = float(user_ping_str)
                    if user_ping > 0:
                        total_price = prediction_per_ping * user_ping
                        total_price_prediction = f"{total_price:,.1f} è¬å…ƒ"
                except (ValueError, TypeError):
                    app.logger.warning(f"ç„¡æ³•å°‡å»ºç‰©ç¸½åªæ•¸ '{user_ping_str}' è½‰æ›ç‚ºæ•¸å­—ã€‚")
                    pass

            result = {
                "status": "é æ¸¬å®Œæˆï¼", 
                "progress": 100, 
                "prediction": f"{prediction_per_ping:,.2f} è¬å…ƒ/åª",
                "total_price": total_price_prediction
            }
            yield f'data: {json.dumps(result)}\n\n'

        except (GeocoderTimedOut, GeocoderServiceError) as geo_e:
            app.logger.error(f"Geocoding error: {geo_e}", exc_info=True)
            error_result = {"error": f"åœ°ç†ç·¨ç¢¼æœå‹™å¤±æ•—ï¼š{geo_e}. è«‹ç¨å¾Œå†è©¦æˆ–æª¢æŸ¥åœ°å€ã€‚"}
            yield f'data: {json.dumps(error_result)}\n\n'
        except ValueError as ve:
            app.logger.error(f"Data processing or validation error: {ve}", exc_info=True)
            error_result = {"error": f"æ•¸æ“šè™•ç†å¤±æ•—ï¼š{str(ve)}"} # ç¢ºä¿ ve å¯ä»¥åºåˆ—åŒ–
            yield f'data: {json.dumps(error_result)}\n\n'
        except Exception as e:
            app.logger.error(f"Unexpected error in prediction stream: {e}", exc_info=True)
            error_result = {"error": f"é æ¸¬å¤±æ•—ï¼šç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ ({type(e).__name__})ï¼Œè«‹æª¢æŸ¥ä¼ºæœå™¨æ—¥èªŒã€‚"}
            yield f'data: {json.dumps(error_result)}\n\n'

    return Response(generate_progress(), mimetype='text/event-stream')

if __name__ == '__main__':
    # å»ºè­°åœ¨é–‹ç™¼æ™‚å•Ÿç”¨ Flask logger
    import logging
    logging.basicConfig(level=logging.DEBUG) # å¯ä»¥çœ‹åˆ° app.logger.debug/info ç­‰è¨Šæ¯
    app.run(debug=True, threaded=True)
